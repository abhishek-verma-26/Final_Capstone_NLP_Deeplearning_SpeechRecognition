{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pipeline for SpeechRecognition Error.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biL8R0oMD5wb",
        "colab_type": "text"
      },
      "source": [
        "## Pipeline for Modeling Speech Recognition Error using NLP and RNN Architecture\n",
        "\n",
        "To summarize the modeling pipeline, here the major steps \n",
        "\n",
        "1. Data Mining: Speech Recognition of Clean and Noisy Audio\n",
        "2. Data Cleaning: Textual Processing using NLP\n",
        "3. Data Wrangling: Speech Error Detect\n",
        "4. Data Selection: Filtering Error Detect Samples\n",
        "5. Data Preparation: Subsampling and Hot Encoding\n",
        "6. Prediction Modelling: RNN and Evaluation\n",
        "7. Output Prediction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtVvZHeWGhOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################\n",
        "################     Data Mining: Speech Recognition of Clean and Noisy Audio     ####################\n",
        "######################################################################################################\n",
        "\n",
        "# Function for transcripting clear speeches\n",
        "def audio_transcript(loc):\n",
        "  harvard = sr.AudioFile(loc)\n",
        "  with harvard as source:\n",
        "    audio = r.record(source)\n",
        "    return r.recognize_google(audio)\n",
        "\n",
        "\n",
        "# Function for transcripting noisy speeches\n",
        "def audio_transcript_all(loc):\n",
        "  harvard = sr.AudioFile(loc)\n",
        "  with harvard as source:\n",
        "    audio = r.record(source)\n",
        "    a = r.recognize_google(audio, show_all=True)\n",
        "    if not a:\n",
        "      return []\n",
        "    else:\n",
        "      c = [inx['transcript'] for inx in a['alternative']]\n",
        "      return c\n",
        "\n",
        "# Clean and Noise audio clip speech to text conversion\n",
        "cnt= 0 \n",
        "for wavfil in trainset_df['file_name']:\n",
        "  cnt  = cnt + 1\n",
        "  cond = trainset_df['file_name'] == wavfil\n",
        "  cln_soundfile = '/content/gdrive/My Drive/clean_trainset/clean_trainset_28spk_wav/'+str(wavfil)+'.wav'\n",
        "  noi_soundfile = '/content/gdrive/My Drive/noisy_trainset/noisy_trainset_28spk_wav/'+str(wavfil)+'.wav'\n",
        "  if not audio_transcript(cln_soundfile):\n",
        "    print (\"Empty Clean Speech Found\")\n",
        "  else:\n",
        "    clean_mat = audio_transcript(cln_soundfile)\n",
        "    trainset_df.loc[cond,'clean_speech'] = clean_mat\n",
        "\n",
        "  noise_mat = audio_transcript_all(noi_soundfile)\n",
        "  cnt1 = 0\n",
        "  for elem in noise_mat:\n",
        "    if (cnt1 < 5):\n",
        "      col = 'noisy_speech_Out'+str(cnt1+1)\n",
        "      trainset_df.loc[cond,col] = elem\n",
        "      cnt1 = cnt1 + 1\n",
        "  print(\"Number of Speech trancripted to text {}\".format(cnt))\n",
        "\n",
        "# Example of empty clean and noise speech translations\n",
        "trainset_df[trainset_df['clean_speech'].isnull()]\n",
        "\n",
        "# Drop empty field in clean speech category\n",
        "cond_clean = ~trainset_df['clean_speech'].isnull()\n",
        "trainset_edit_df = trainset_df[cond_clean]\n",
        "trainset_edit_df = trainset_edit_df.reset_index()\n",
        "trainset_edit_df = trainset_edit_df.drop(['index'],axis=1)\n",
        "trainset_edit_df\n",
        "\n",
        "######################################################################################################\n",
        "################     Data Cleaning: Textual Processing using NLP     #################################\n",
        "######################################################################################################\n",
        "\n",
        "# to convert numbers in words\n",
        "!pip install inflect\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        "\n",
        "# Spacy Creating Document\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# all the processing work is done below, so it may take a while\n",
        "for rev in noise_output_df['clean_speech']:\n",
        "  text =  rev.replace(\"\\'\",\"\").replace(\"/\",\" by \")\n",
        "  review_doc = nlp(text)\n",
        "  c = []\n",
        "  for token in review_doc:\n",
        "    if not token.is_punct:\n",
        "      if not token.is_space:\n",
        "        if token.is_alpha:\n",
        "          c.append(token.lower_)\n",
        "        elif token.is_digit:\n",
        "          c.append(p.number_to_words(token))\n",
        "  noise_output_df.loc[noise_output_df['clean_speech'] == rev,'clean_speech_edit']= \" \".join(c)\n",
        "\n",
        "# all the processing work is done below, so it may take a while\n",
        "for rev in noise_output_df['noisy_speech']:\n",
        "  text =  rev.replace(\"\\'\",\"\").replace(\"/\",\" by \")\n",
        "  review_doc = nlp(text)\n",
        "  c = []\n",
        "  for token in review_doc:\n",
        "    if not token.is_punct:\n",
        "      if not token.is_space:\n",
        "        if token.is_alpha:\n",
        "          c.append(token.lower_)\n",
        "        elif token.is_digit:\n",
        "          c.append(p.number_to_words(token))\n",
        "  noise_output_df.loc[noise_output_df['noisy_speech'] == rev,'noisy_speech_edit']= \" \".join(c)\n",
        "\n",
        "######################################################################################################\n",
        "################     Data Wrangling: Speech Error Detect                             #################\n",
        "######################################################################################################\n",
        "\n",
        "def word_mismatch2(mat1,mat2, nb):\n",
        "  totn_mat1 = []\n",
        "  disn_mat1 = [] \n",
        "  # Find similar words\n",
        "  cnt = 0\n",
        "  for ap in mat1:\n",
        "    lim1 = max(0,cnt-nb)\n",
        "    lim2 = min(cnt+nb,len(mat2))\n",
        "    totn_mat1.append([ind for ind in mat2[lim1:lim2] if (ind == ap)])\n",
        "    cnt = cnt + 1\n",
        "  #print(totn_mat1)\n",
        "\n",
        "  # Filter mismatched Words\n",
        "  cnt = 0\n",
        "  misnum = 0\n",
        "  for at in totn_mat1:\n",
        "    if (at == []):\n",
        "      disn_mat1.append(mat1[cnt])\n",
        "      misnum = misnum + 1\n",
        "    else:\n",
        "      disn_mat1.append('xxxxxx')\n",
        "    cnt = cnt + 1\n",
        "\n",
        "  outp = \" \".join(disn_mat1)\n",
        "  if len(outp) == 0 :\n",
        "    outp = 'xxxxxx'\n",
        "  return {'sent': outp, 'num_word' : misnum}\n",
        "\n",
        "# clean / noisy edit\n",
        "for ind in range(0,noise_output_df.shape[0]):\n",
        "  a_mat = [token.lower_ for token in nlp(noise_output_df.iloc[ind]['clean_speech_edit'])]\n",
        "  b_mat = [token.lower_ for token in nlp(noise_output_df.iloc[ind]['noisy_speech_edit'])]\n",
        "  noise_output_df.loc[ind,'clean_speech_edit_detect'] = word_mismatch2(a_mat,b_mat,2)['sent']\n",
        "  noise_output_df.loc[ind,'noisy_speech_edit_detect'] = word_mismatch2(b_mat,a_mat,2)['sent']\n",
        "  noise_output_df.loc[ind,'clean_speech_edit_detect_num'] = word_mismatch2(a_mat,b_mat,2)['num_word']\n",
        "  noise_output_df.loc[ind,'noisy_speech_edit_detect_num'] = word_mismatch2(b_mat,a_mat,2)['num_word']\n",
        "\n",
        "######################################################################################################\n",
        "################     Data Selection: Filtering Error Detect Samples                  #################\n",
        "######################################################################################################\n",
        "\n",
        "# Choose instances where only one word was errored\n",
        "trainset_noise_df = trainset_noise_df[(noise_output_df['noisy_speech_edit_detect_num'] == 1) & (noise_output_df['clean_speech_edit_detect_num'] == 1) ]\n",
        "trainset_noise_df = trainset_noise_df.reset_index()\n",
        "trainset_noise_df = trainset_noise_df.drop(['index'],axis=1)\n",
        "\n",
        "######################################################################################################\n",
        "################     Data Prep: Subsampling and Hot Encoding         #################################\n",
        "######################################################################################################\n",
        "\n",
        "def input_split(text, veclen):\n",
        "  inpmat = []\n",
        "  nlp_text = [token.lower_ for token in nlp(text)]\n",
        "  nlp_text = [\"#\"]+nlp_text+[\"#\"]\n",
        "  for ind in range(0,veclen):\n",
        "    if ind < (len(nlp_text)-1):\n",
        "      if (nlp_text[ind] != '#'):\n",
        "        inpmat.append(\" \".join([nlp_text[ind-1],nlp_text[ind],nlp_text[ind+1]]))\n",
        "  return inpmat\n",
        "\n",
        "def output_mat_split(text, origtext, veclen):\n",
        "  outmat = []\n",
        "  nlp_text     = [token.lower_ for token in nlp(text)]\n",
        "  nlp_origtext = [token.lower_ for token in nlp(origtext)]\n",
        "  nlp_text = [\"#\"]+nlp_text+[\"#\"]\n",
        "  nlp_origtext = [\"#\"]+nlp_origtext+[\"#\"]\n",
        "  for ind in range(0,veclen):\n",
        "    if ind < (len(nlp_text)-1):\n",
        "      if (nlp_text[ind] != '#'):\n",
        "        if ((((nlp_text[ind-1] == 'xxxxxx') | (nlp_text[ind-1] == '#')) & (nlp_text[ind] != 'xxxxxx') & ((nlp_text[ind+1] == 'xxxxxx') | (nlp_text[ind+1] == '#')))):\n",
        "          outmat.append([0,0,1,0])\n",
        "        elif ((((nlp_text[ind-1] != 'xxxxxx') & (nlp_text[ind-1] != '#')) & (nlp_text[ind] == 'xxxxxx') & ((nlp_text[ind+1] == 'xxxxxx') | (nlp_text[ind+1] == '#')))):\n",
        "          outmat.append([0,1,0,0])\n",
        "        elif ((((nlp_text[ind-1] == 'xxxxxx') | (nlp_text[ind-1] == '#')) & (nlp_text[ind] == 'xxxxxx') & ((nlp_text[ind+1] != 'xxxxxx') & (nlp_text[ind+1] != '#')))):\n",
        "          outmat.append([0,0,0,1])\n",
        "        elif ((((nlp_text[ind-1] == 'xxxxxx') | (nlp_text[ind-1] == '#')) & (nlp_text[ind] != 'xxxxxx') & ((nlp_text[ind+1] != 'xxxxxx') & (nlp_text[ind+1] != '#')))):\n",
        "          outmat.append([num/2.0 for num in [0,0,1,1]])\n",
        "        elif ((((nlp_text[ind-1] != 'xxxxxx') & (nlp_text[ind-1] != '#')) & (nlp_text[ind] == 'xxxxxx') & ((nlp_text[ind+1] != 'xxxxxx') & (nlp_text[ind+1] != '#')))):\n",
        "          outmat.append([num/2.0 for num in [0,1,0,1]])\n",
        "        elif ((((nlp_text[ind-1] != 'xxxxxx') & (nlp_text[ind-1] != '#')) & (nlp_text[ind] != 'xxxxxx') & ((nlp_text[ind+1] == 'xxxxxx') | (nlp_text[ind+1] == '#')))):\n",
        "          outmat.append([num/2.0 for num in [0,1,1,0]])\n",
        "        elif ((((nlp_text[ind-1] != 'xxxxxx') & (nlp_text[ind-1] != '#')) & (nlp_text[ind] != 'xxxxxx') & ((nlp_text[ind+1] != 'xxxxxx') & (nlp_text[ind+1] != '#')))):\n",
        "          outmat.append([num/3.0 for num in [0,1,1,1]])\n",
        "        else:\n",
        "          outmat.append([1,0,0,0])\n",
        "  return outmat\n",
        "\n",
        "# Preparing three string inputs for Neural Network using Subsampling\n",
        "nn_input_mat = []\n",
        "for i,_ in enumerate(trainset_noise_df['noisy_speech_edit']):\n",
        "  orig_text   = trainset_noise_df.iloc[i]['noisy_speech_edit']\n",
        "  detect_text = trainset_noise_df.iloc[i]['noisy_speech_edit_detect']\n",
        "  nn_input_mat  = np.append(nn_input_mat,input_split(orig_text,100), axis = 0)\n",
        "\n",
        "# Preparing dummy array output for Neural Network using Subsampling\n",
        "nn_output_loc_mat = []\n",
        "for i,_ in enumerate(trainset_noise_df['noisy_speech_edit']):\n",
        "  orig_text   = trainset_noise_df.iloc[i]['noisy_speech_edit']\n",
        "  detect_text = trainset_noise_df.iloc[i]['noisy_speech_edit_detect']\n",
        "  if (i == 0) :\n",
        "    nn_output_loc_mat = output_mat_split(detect_text,orig_text,100)\n",
        "  else :\n",
        "    nn_output_loc_mat = np.append(nn_output_loc_mat,output_mat_split(detect_text,orig_text,100), axis = 0)\n",
        "\n",
        "from keras.preprocessing import text\n",
        "from keras.preprocessing import sequence\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "\n",
        "tokenizer_inp = text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"$%&()*+,-/:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "#tokenizer.fit_on_texts(np.append(rhym_comb, targ_comb, axis=0))\n",
        "tokenizer_inp.fit_on_texts(nn_input_mat)\n",
        "word_index_inp = tokenizer_inp.word_index\n",
        "print('Found %s unique tokens.' % len(word_index_inp))\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 3\n",
        "X = tokenizer_inp.texts_to_sequences(nn_input_mat)\n",
        "X = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "#X = X/len(word_index_inp)\n",
        "X=X.astype('float')\n",
        "Y= nn_output_loc_mat#.astype('float')\n",
        "\n",
        "print('Shape of data X tensor:', X.shape)\n",
        "print('Shape of data Y tensor:', Y.shape)\n",
        "\n",
        "######################################################################################################\n",
        "################       Prediction Modelling : RNN Modelling and Evaluation            ################\n",
        "###################################################################################################### \n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, SpatialDropout1D\n",
        "from keras import optimizers\n",
        "\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 6000\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 16\n",
        "# Number of Epoch to Test\n",
        "epochs = 10\n",
        "# Batch Size of NN\n",
        "batch_size = 320\n",
        "\n",
        "# Defining the NN Model Architecture (Learning Rate : 0.01)\n",
        "model_pl = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]),\n",
        "    tf.keras.layers.SpatialDropout1D(0.2),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(8)),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "adam = optimizers.Adam(learning_rate=0.01)\n",
        "model_pl.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_pl.summary()\n",
        "\n",
        "# Fitting the Model on the current data\n",
        "history_p1 = model_p1.fit(X, Y, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.1, verbose=1)\n",
        "\n",
        "######################################################################################################\n",
        "################     Output Prediction                               #################################\n",
        "######################################################################################################\n",
        "\n",
        "def pred_conv(input,nnmodel):\n",
        "  for i,text in enumerate(input):\n",
        "      nlp_text = [token.lower_ for token in nlp(text)]\n",
        "      nlp_text = [\"#\"]+nlp_text+[\"#\"]\n",
        "      inpmat = []\n",
        "      txtmat = []\n",
        "      for ind in range(1,len(nlp_text)-1):\n",
        "        inpmat.append(\" \".join([nlp_text[ind-1],nlp_text[ind],nlp_text[ind+1]]))\n",
        "        txtmat.append(nlp_text[ind])\n",
        "      #print(inpmat)\n",
        "      \n",
        "      # Sequence tokenizing\n",
        "      MAX_SEQUENCE_LENGTH = 3\n",
        "      token_inpmat = tokenizer_inp.texts_to_sequences(inpmat)\n",
        "      token_inpmat = sequence.pad_sequences(token_inpmat, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "      #print(token_inpmat)\n",
        "      prb_outmat = nnmodel.predict(token_inpmat)\n",
        "      #print(prb_outmat)\n",
        "      outmat = [np.argmax(submat) for submat in prb_outmat]\n",
        "      #print(txtmat)\n",
        "      #print(outmat)\n",
        "      cnt_word = []\n",
        "      fnl_mat = []\n",
        "      for i,rt in enumerate(outmat) :\n",
        "        #print(i)\n",
        "        if (rt == 3):\n",
        "          cnt_word.append(nlp_text[i+2])\n",
        "        elif (rt == 2):\n",
        "          cnt_word.append(nlp_text[i+1])\n",
        "        elif (rt == 1):\n",
        "          cnt_word.append(nlp_text[i])\n",
        "      uniq_cnt_word = np.unique(cnt_word)      \n",
        "      for wrd in uniq_cnt_word:\n",
        "        #print(text,\"\\t---------->\\t\",wrd,\" with probability \",round(cnt_word.count(wrd)/3,2))\n",
        "        fnl_mat.append({wrd : round(cnt_word.count(wrd)/3,2)})\n",
        "      if len(uniq_cnt_word) == 0 :\n",
        "        fnl_mat = [\"NO ERROR DETECTED\"]\n",
        "      print(text,\"\\t---------->\\t\",fnl_mat)\n",
        "  return \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}